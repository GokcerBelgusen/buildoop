<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->
<configuration>

        <property>
                <name>dfs.namenode.name.dir</name>
                <value>/cluster/metadata/dfs/nn</value>
                <final>true</final>
                <description>Determines where on the local filesystem the DFS name node 
                    should store the name table(fsimage). If this is a comma-delimited
                    list of directories then the name table is replicated in all of 
                    the directories, for redundancy</description>
        </property>

        <property>
                <name>dfs.datanode.data.dir</name>
                <value>/cluster/data/1/dfs/dn</value>
                <final>true</final>
                <description>Determines where on the local filesystem an DFS data node 
                    should store its blocks. If this is a comma-delimited list of directories,
                    then data will be stored in all named directories, typically on different
                    devices. Directories that do not exist are ignored. This property specifies 
                    the directories where the DataNode stores blocks. The recommendation is 
                    that you configure the disks on the DataNode in a JBOD configuration, 
                    mounted at /data/1/ through /data/N, and configure dfs.datanode.data.dir 
                    to specify /data/1/dfs/dn through /data/N/dfs/dn/.</description>
        </property>

    <!-- HDFS HA Configurations -->

    <property>
        <name>dfs.nameservices</name>
        <value>buildoopcluster</value>
        <description>logical name for this nameservice</description>
    </property>

    <property>
        <name>dfs.ha.namenodes.buildoopcluster</name>
        <value>nn1,nn2</value>
        <description>list of comma-separated NameNode IDs</description>
    </property>

    <property>
        <name>dfs.namenode.rpc-address.buildoopcluster.nn1</name>
        <value>hadoop-manager.buildoop.org:8020</value>
        <description>The fully-qualified RPC address for each NameNode
            to listen on</description>
    </property>

    <property>
        <name>dfs.namenode.rpc-address.buildoopcluster.nn2</name>
        <value>hadoop-node1.buildoop.org:8020</value>
        <description>The fully-qualified RPC address for each NameNode
            to listen on</description>
    </property>

    <property>
        <name>dfs.namenode.http-address.buildoopcluster.nn1</name>
        <value>hadoop-manager.buildoop.org:50070</value>
        <description>The fully-qualified HTTP address for each NameNode
            to listen on</description>
    </property>

    <property>
        <name>dfs.namenode.http-address.buildoopcluster.nn2</name>
        <value>hadoop-node1.buildoop.org:50070</value>
        <description>The fully-qualified HTTP address for each NameNode
            to listen on</description>
    </property>

    <property>
        <name>dfs.namenode.shared.edits.dir</name>
        <value>
        qjournal://hadoop-manager.buildoop.org:8485;hadoop-node1.buildoop.org:8485;hadoop-node2.buildoop.org:8485/buildoopcluster
        </value>
        <description>This is the The location of the shared storage directory.
            You must specify several JournalNode addresses:
            The machine architecture recommended is put one JournalNode 
            daemon in each NameNode (the Active and the Standby) and in the YARN 
            ResourceManager. So 3 JournalNode daemons are recommended.</description>
    </property>

    <property>
        <name>dfs.journalnode.edits.dir</name>
        <value>/cluster/data/1/dfs/jn</value>
        <description>The path where the JournalNode daemon will store its 
            local state. On each JournalNode machine.</description>
    </property>

    <property>
        <name>dfs.client.failover.proxy.provider.buildoopcluster</name>
        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
        <description>The Java class that HDFS clients use to contact the Active NameNode.</description>
    </property> 

    <property>
        <name>dfs.ha.fencing.methods</name>
        <value>shell(/bin/true)</value>
    </property>

    <!-- HDFS HA automatic failover with Zookeeper -->

    <property>
        <name>dfs.ha.automatic-failover.enabled</name>
        <value>true</value>
        <description>This specifies that the cluster should be set up for automatic failover</description>
    </property>

	<!-- Hadoop Secure Mode - KERBEROS -->

	<!-- General HDFS security config -->

     <property>
        <name>dfs.block.access.token.enabled</name>
        <value>true</value>
        <description>Block access tokens are temporary keys that allow an HDFS block to be read,
        written, deleted, or a host of other internal actions, by an authenticated user. This
        mechanism allows Hadoop to ensure that only the intended users are able to access
        data in HDFS. While disabled (false) by default, this parameter should be enabled
        (set to true) in a secure deployment.
        </description>
    </property> 

	<!-- NameNode security config -->

     <property>
        <name>dfs.namenode.keytab.file</name>
        <value>/etc/hadoop/conf/security/hdfs.keytab</value>
        <description>specifies the location of the keytab that
        contains the Kerberos principal key for the namenode. This is the file uploaded to
        each host and by convention, is placed in the Hadoop configuration directory. This 
        is the path to the HDFS keytab.
        </description>
    </property> 

     <property>
        <name>dfs.namenode.kerberos.principal</name>
        <value>hdfs/_HOST@BUILDOOP.ORG</value>
        <description>The Kerberos principal the namenode should use to authenticate. The key for this
        principal must exist in the keytab specified by dfs.namenode.keytab.file. The special token 
        _HOST can be used for the instance portion of the principal, in which case the fully 
        qualified domain name will be interpolated. Note that the _HOST
        token cannot be used anywhere else in the principal.
        </description>
    </property> 

     <property>
        <name>dfs.namenode.kerberos.internal.spnego.principal</name>
        <value>HTTP/_HOST@BUILDOOP.ORG</value>
    </property> 

     <!-- DataNode security config -->

     <property>
        <name>dfs.datanode.data.dir.perm</name>
        <value>700</value>
        <description>When security is enabled, Hadoop performs extra checks to ensure HDFS block
        data cannot be read by unauthorized users. One of these checks involves making
        sure the directories specified by dfs.data.dir are set to restrictive permissions. This
        prevents user code from simply opening and reading block data directly from the
        local disk rather than using the HDFS APIs, which require a valid Kerberos ticket
        and perform authorization checks on the file. If the permissions are incorrect, the
        datanode will change the permissions to the value specified by this parameter.
        </description>
    </property> 

    <property>
        <name>dfs.datanode.address</name>
        <value>0.0.0.0:1004</value>
        <description>The hostname or IP address and port, separated by a colon, on which the data
        transceiver RPC server should be bound. It is valid to specify the wild card IP 0.0.0.0
        to indicate the server should listen on all interfaces. With security enabled, this
        port must be below 1024 or the datanode will not start.
        </description>
    </property> 

     <property>
        <name>dfs.datanode.http.address</name>
        <value>0.0.0.0:1006</value>
        <description>The hostname or IP address and port, separated by a colon, on which the embedded
        HTTP server should be bound. It is valid to specify the wild card IP 0.0.0.0 to
        indicate the HTTP server should listen on all interfaces.
        </description>
    </property> 

    <property>
        <name>dfs.datanode.keytab.file</name>
        <value>/etc/hadoop/conf/security/hdfs.keytab</value>
        <description>Exactly the same as dfs.namenode.keytab.file, the dfs.datanode.keytab.file
        specifies the keytab file containing the principal keys used by the datanode process.
        This can, and usually is, the same file as dfs.namenode.keytab.file.
        </description>
    </property> 

     <property>
        <name>dfs.datanode.kerberos.principal</name>
        <value>hdfs/_HOST@BUILDOOP.ORG</value>
        <description>The Kerberos principal the datanode should use to authenticate. The key for this
        principal must exist in the keytab specified by dfs.datanode.keytab.file. The spe-
        cial token _HOST can be used for the instance portion of the principal, in which
        case the fully qualified domain name will be interpolated. Note that the _HOST
        token cannot be used anywhere else in the principal. This is commonly the same
        principal as dfs.namenode.kerberos.principal.
        </description>
    </property> 

    <!-- security for Quorum-based Storage -->

    <property>
        <name>dfs.journalnode.keytab.file</name>
        <value>/etc/hadoop/conf/security/hdfs.keytab</value>
    </property> 
     <property>
        <name>dfs.journalnode.kerberos.principal</name>
        <value>hdfs/_HOST@BUILDOOP.ORG</value>
    </property> 
    <property>
        <name>dfs.journalnode.kerberos.internal.spnego.principal</name>
        <value>HTTP/_HOST@BUILDOOP.ORG</value>
    </property> 

     <!-- Secure WebHDFS -->

     <property>
        <name>dfs.webhdfs.enabled</name>
        <value>true</value>
    </property> 

     <property>
        <name>dfs.web.authentication.kerberos.principal</name>
        <value>HTTP/_HOST@BUILDOOP.ORG</value>
    </property> 

    <property>
        <name>dfs.web.authentication.kerberos.keytab</name>
        <value>/etc/hadoop/conf/security/HTTP.keytab</value>
    </property> 

</configuration>
